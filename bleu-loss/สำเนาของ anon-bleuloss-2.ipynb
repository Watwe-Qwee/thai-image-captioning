{"cells":[{"cell_type":"markdown","metadata":{"id":"Yxbrd9jOyN2E"},"source":["The DBLEU Loss follow this paper : https://openreview.net/forum?id=S1x2aiRqFX "]},{"cell_type":"markdown","metadata":{"id":"RmFk4O182tu5"},"source":["#Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":911,"status":"ok","timestamp":1652970287032,"user":{"displayName":"Chawakorn Phiantham","userId":"05119468888744169723"},"user_tz":-420},"id":"ge0_ca6A88Vn","outputId":"6791395d-b88d-4b65-d9f6-4e8c6c16c0ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu May 19 14:24:45 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24720,"status":"ok","timestamp":1652970312454,"user":{"displayName":"Chawakorn Phiantham","userId":"05119468888744169723"},"user_tz":-420},"id":"Jqsy9Fk32hyw","outputId":"0ffa0631-1c78-436c-c6b0-6a889261ee49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10337,"status":"ok","timestamp":1652970322782,"user":{"displayName":"Chawakorn Phiantham","userId":"05119468888744169723"},"user_tz":-420},"id":"dDf-wD422hc1","outputId":"679fed6c-e8a1-4357-ac92-4fcfc57e6538"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pythainlp\n","  Downloading pythainlp-3.0.8-py3-none-any.whl (11.5 MB)\n","\u001b[K     |████████████████████████████████| 11.5 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n","Collecting tinydb>=3.0\n","  Downloading tinydb-4.7.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2021.10.8)\n","Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp) (4.2.0)\n","Installing collected packages: tinydb, pythainlp\n","Successfully installed pythainlp-3.0.8 tinydb-4.7.0\n","Collecting torchinfo\n","  Downloading torchinfo-1.6.6-py3-none-any.whl (21 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.6.6\n","Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.11.0+cu113)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.2.0)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=a7b3ea4b407d985d021dd5e01a3982320bebbde7a630fe3bc787ad034d8b2d0e\n","  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]}],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/CP-Eng-3rd-year/Pattern Recog/Project/00 Library/')\n","!pip install --upgrade pythainlp\n","!pip install torchinfo\n","!pip install torchviz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoYYm4uX2yLh","colab":{"base_uri":"https://localhost:8080/","height":322},"executionInfo":{"status":"error","timestamp":1652970322788,"user_tz":-420,"elapsed":23,"user":{"displayName":"Chawakorn Phiantham","userId":"05119468888744169723"}},"outputId":"7cc9d1fe-d86b-4938-9411-b9f373420c48"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-33338b2089c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_processing_non\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_processing_non'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from data_processing_non import dictionary\n","from model_dataloader import EncoderDecoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtRjiJDw24-v"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm.notebook import tqdm \n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import random\n","\n","\n","import torch\n","from torch import nn \n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.models as models\n","import torch.nn.functional as F\n","from torchvision import transforms, models\n","from torch.nn.utils.rnn import pad_sequence\n","from torchinfo import summary\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7t6k21G-RREL"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"Bl7o72jvzDW0"},"source":["#Data set and Data loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHXnl8qd60bD"},"outputs":[],"source":["def get_identical_matrix(vector):\n","  identical_matrix = torch.eye(len(vector))\n","\n","  for i in range(len(vector)):\n","    for j in range(len(vector)):\n","      identical_matrix[i, j] = vector[i] == vector[j]\n","\n","  return identical_matrix\n","\n","def get_n_gram_hadamard_operation(matrix, n = 4):\n","  outputs = []\n","  matrix_n = matrix\n","  outputs.append(matrix)\n","\n","  w = matrix.shape[0]\n","  h = matrix.shape[1]\n","\n","  for i in range(1, n):\n","    if w-i <= 0 or h-i <= 0:\n","      break\n","    matrix_n = matrix[:-i, :-i] * matrix_n[1:, 1:]\n","    outputs.append(matrix_n)\n","\n","  return outputs \n","\n","def get_similarity_matrix(pred, caption):\n","  T = len(caption)\n","  output = torch.zeros(T, T)\n","  for i in range(T):\n","    for j in range(T):\n","      output[i, j] = pred[i, caption[j]].contiguous()\n","  return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMfwjW3WoV5I"},"outputs":[],"source":["class FlickrDataset_Bleu_loss(Dataset):\n","    def __init__(self, \n","                 captions_files, \n","                 image_dir = \"/content/drive/MyDrive/CP-Eng-3rd-year/Pattern Recog/Project/00 Data/archive/Images/\", \n","                 transform=None,\n","                 dictionary=dictionary,\n","                 test = False):\n","      \n","      self.image_dir = image_dir\n","\n","      if type(captions_files) is str:\n","        self.dfs = pd.read_csv(captions_files)\n","        self.dfs = [self.df]\n","      elif type(captions_files) is list:\n","        for i in range(len(captions_files)):\n","          if  type(captions_files[i]) is str:\n","            captions_files[i] = pd.read_csv(captions_files[i])\n","        self.dfs = captions_files\n","\n","      self.transform = transform\n","      self.test = test\n","      self.img_id = []\n","      self.img = {}\n","      self.caption = {}\n","      self.dictionary = dictionary\n","\n","      self.caption_n_gram_count = {}\n","\n","      unique_image = self.dfs[0]['image'].unique()\n","\n","      for i in tqdm(range(len(unique_image))):\n","        img_id = unique_image[i]\n","        if img_id in self.caption:\n","          continue\n","        self.img_id.append(img_id)\n","        self.caption[img_id] = []\n","        self.caption_n_gram_count[img_id] = []\n","      \n","      for df in self.dfs:\n","\n","        for i in tqdm(range(len(df))):\n","          img_id = df.iloc[i]['image']\n","          caption = df.iloc[i]['tokenized_caption_th']\n","\n","          caption = caption[1:-1]\n","          caption = caption.split(\",\")\n","          caption = [word[1:].replace(\"'\", \"\") for word in caption] \n","          caption = self.dictionary.numericalize(caption)\n","          caption = torch.tensor(caption)\n","          \n","          identical_matrix = get_identical_matrix(caption[1:-1])\n","          n_gram_indentical_matrices = get_n_gram_hadamard_operation(identical_matrix)\n","          n_gram_count = [torch.sum(matrix, dim = 1) for matrix in n_gram_indentical_matrices]\n","          self.caption[img_id].append(caption)\n","          self.caption_n_gram_count[img_id].append(n_gram_count)\n","        \n","        \n","    def __len__(self):\n","      return len(self.img_id)\n","    \n","    def __getitem__(self, idx): \n","\n","      img_id = self.img_id[idx]\n","      if img_id not in self.img:\n","        img_path = os.path.join(self.image_dir, img_id)\n","        img = Image.open(img_path).convert(\"RGB\")\n","        self.img[img_id] = img\n","\n","      img = self.img[img_id]\n","      randomized_index = random.randint(0, len(self.caption[img_id])-1)\n","      caption = self.caption[img_id][randomized_index]\n","      caption_n_gram_count = self.caption_n_gram_count[img_id][randomized_index]\n","\n","      if self.transform is not None:\n","        transformed_img = self.transform(img)\n","\n","      if not self.test:\n","        return transformed_img, caption, caption_n_gram_count, img\n","\n","      return transformed_img, caption, caption_n_gram_count, img, self.caption[img_id]\n","      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh1zrnBkzAHD"},"outputs":[],"source":["# Collate_fn for calling to transform a batch before return. \n","class Collate_fn:\n","  def __init__(self, pad_value, batch_first=False):\n","    self.pad_value = pad_value       \n","    self.batch_first = batch_first    \n","        \n","  def __call__(self, batch):\n","    transformed_imgs = [item[0].unsqueeze(0) for item in batch]\n","    transformed_imgs = torch.cat(transformed_imgs, dim=0)    \n","        \n","    captions = [item[1] for item in batch]   \n","    captions = pad_sequence(captions, batch_first = self.batch_first, padding_value = self.pad_value)\n","    \n","    if len(batch[0]) > 4:\n","      return transformed_imgs, captions, [item[2] for item in batch], [item[3] for item in batch], [item[4] for item in batch]\n","\n","    return transformed_imgs, captions, [item[2] for item in batch], [item[3] for item in batch]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVYhdc7J3Cxf"},"outputs":[],"source":["dataframe_path = \"/content/drive/MyDrive/CP-Eng-3rd-year/Pattern Recog/Project/00 Data/archive/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpBTrCrJ2MrT"},"outputs":[],"source":["train_captions_files = [dataframe_path+\"captions_train.csv\"]\n","val_captions_files = [dataframe_path+\"captions_val.csv\"]\n","test_captions_files = [dataframe_path+\"captions_test.csv\"]\n","\n","image_size = 299\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.CenterCrop(image_size),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                          std=[0.5, 0.5, 0.5])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.CenterCrop(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                          std=[0.5, 0.5, 0.5])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfnTEezNzCin"},"outputs":[],"source":["batch_size = 32\n","train_flickr_bleu_dataset = FlickrDataset_Bleu_loss(transform = train_transform, captions_files = train_captions_files)\n","train_flickr_bleu_dataloader = torch.utils.data.DataLoader( train_flickr_bleu_dataset, \n","                                                batch_size=batch_size, \n","                                                shuffle=True, \n","                                                pin_memory=True, \n","                                                collate_fn = Collate_fn(pad_value=0, batch_first = True)) \n","val_flickr_bleu_dataset = FlickrDataset_Bleu_loss(transform = val_transform, captions_files = val_captions_files)\n","val_flickr_bleu_dataloader = torch.utils.data.DataLoader( val_flickr_bleu_dataset, \n","                                                batch_size=batch_size, \n","                                                shuffle=True, \n","                                                pin_memory=True, \n","                                                collate_fn = Collate_fn(pad_value=0, batch_first = True)) \n","test_flickr_bleu_dataset = FlickrDataset_Bleu_loss(transform = val_transform, captions_files = test_captions_files, test = True)\n","test_flickr_bleu_dataloader = torch.utils.data.DataLoader( test_flickr_bleu_dataset, \n","                                                batch_size=batch_size, \n","                                                shuffle=True, \n","                                                pin_memory=True, \n","                                                collate_fn = Collate_fn(pad_value=0, batch_first = True)) "]},{"cell_type":"code","source":["len(next(iter(test_flickr_bleu_dataloader)))"],"metadata":{"id":"M8dfa-rUW_w0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W08F1WYpQswg"},"source":["#Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiWW8KvzQt9N"},"outputs":[],"source":["embed_size=300\n","vocab_size = len(dictionary.stoi)\n","attention_dim=256\n","encoder_dim=2048\n","decoder_dim=512\n","learning_rate = 3e-4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M035mdwbQ0OI"},"outputs":[],"source":["transformed_img, captions, caption_n_gram_count, img = next(iter(train_flickr_bleu_dataloader))\n","transformed_img, captions = transformed_img.to(device), captions.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Yzc6a_mRMAq"},"outputs":[],"source":["model = EncoderDecoder(embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_D90kykJVX1"},"outputs":[],"source":["#best_path = \"/content/drive/MyDrive/CP-Eng-3rd-year/Pattern Recog/Project/01 Model/Anon/baseline_attention_cross_ling_model_state.pth\"\n","#best = torch.load(best_path)\n","#model.load_state_dict(best['state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"oJCO6whKNKh-"},"source":["#BLEU Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCxeplhwB7I2"},"outputs":[],"source":["from torch.autograd import Variable\n","eps = 1e-6\n","class BLEU_loss(nn.Module):\n","  def __init__(self, N_max = 4):\n","    super().__init__()\n","    self.N_max = N_max \n","        \n","  def forward(self, preds, captions, Cs):\n","    prec = torch.zeros(len(preds))\n","    BP = torch.zeros(len(preds))\n","\n","    for index in range(len(preds)):\n","      caption = captions[index]\n","\n","      stop_word_index = (caption == 2).nonzero()[0]\n","      caption = caption[1:stop_word_index]\n","      T = len(caption)\n","\n","      pred = preds[index]\n","      text_output = torch.argmax(pred, dim = 1)\n","      if 2 in text_output:\n","        stop_word_index_pred = (text_output == 2).nonzero()[0]\n","        text_output = text_output[:stop_word_index_pred]\n","      else :\n","        text_output = text_output\n","      pred = pred[:stop_word_index]\n","      pred = nn.functional.softmax(pred, dim=1)\n","\n","      similarity_matrix = get_similarity_matrix(pred, caption)\n","      Ms = get_n_gram_hadamard_operation(similarity_matrix) \n","\n","      C = Cs[index]\n","      Os = torch.tensor([0.0 for _ in range(self.N_max)])\n","\n","      for n in range(self.N_max):\n","        if n >= len(Ms):\n","          break\n","        M = Ms[n]\n","        diag = torch.diagonal(M, 0)\n","        temp = 1 + torch.sum(M, dim = 1) - diag \n","\n","        appearance_number = C[n]\n","        min_part = appearance_number/temp\n","        min_part = torch.clamp(min_part, max = 1)\n","\n","        #print(M.shape, appearance_number.shape, min_part.shape)\n","        temp = (M.T/appearance_number*min_part).T\n","        temp = torch.sum(temp)\n","        Os[n] = torch.log(temp/(T-n))\n","\n","      number_word_real = len(caption)\n","      number_word_pred = len(text_output)\n","      if number_word_real > number_word_pred:\n","        BP[index] = -torch.log(torch.exp(torch.tensor(1-number_word_real/(number_word_pred+eps))))\n","      prec[index] = -torch.sum(Os)/len(Ms)\n","      \n","\n","    BP = torch.mean(BP) \n","    prec = torch.mean(prec) \n","\n","    return BP+prec"]},{"cell_type":"markdown","metadata":{"id":"V9fdSj_oEf2-"},"source":["#Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsSA2f_uEo8h"},"outputs":[],"source":["num_epoch = 25"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr_A7i8EGFXh"},"outputs":[],"source":["bleu_loss_fn = BLEU_loss()\n","criterion = nn.CrossEntropyLoss(ignore_index=dictionary.stoi[\"<PAD>\"])\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 0, min_lr = 1e-7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TS5M3qcyFigw"},"outputs":[],"source":["best_path = \"/content/drive/MyDrive/CP-Eng-3rd-year/Pattern Recog/Project/01 Model/Anon/bleu_loss_model_state.pth\"\n","\n","def save_model(model,num_iteration):\n","    model_state = {\n","        'num_epochs':num_iteration,\n","        'embed_size':embed_size,\n","        'vocab_size':vocab_size,\n","        'attention_dim':attention_dim,\n","        'encoder_dim':encoder_dim,\n","        'decoder_dim':decoder_dim,\n","        'state_dict':model.state_dict()\n","    }\n","    torch.save(model_state, best_path)\n","    print(best_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwX2FUJAEhjU"},"outputs":[],"source":["import math\n","\n","best_val = float('inf')\n","lrs = []\n","train_losses = []\n","val_losses = []\n","\n","\n","\n","for i in tqdm(range(num_epoch)): \n","\n","  model.train()\n","  total_train_loss = 0 \n","  count = 0 \n","  weight = 0.35*math.exp(-i/10)\n","  print(0.65+weight, 0.35-weight)\n","\n","  for transformed_img, captions, caption_n_gram_count, _ in tqdm(train_flickr_bleu_dataloader):\n","  \n","    optimizer.zero_grad()\n","    transformed_img, captions = transformed_img.to(device), captions.to(device)\n","\n","    outputs, attentions = model(transformed_img, captions)\n","    #bleu_loss = bleu_loss_fn(outputs, captions, caption_n_gram_count)\n","    criterion_loss = criterion(outputs.reshape(-1, vocab_size), captions[:,1:].reshape(-1))\n","\n","    train_loss = criterion_loss \n","    train_loss.backward()\n","\n","    total_train_loss += train_loss\n","    count+= 1\n","    optimizer.step()\n","\n","  total_train_loss /= count\n","  scheduler.step(total_train_loss)\n","  lrs.append(optimizer.param_groups[0]['lr'])\n","  train_losses.append(train_loss.item())\n","\n","  model.eval()\n","  total_val_loss = 0 \n","  count = 0 \n","  with torch.no_grad():\n","    for transformed_img, captions, caption_n_gram_count, _ in val_flickr_bleu_dataloader:\n","      transformed_img, captions = transformed_img.to(device), captions.to(device)\n","      outputs, attentions = model(transformed_img, captions)\n","      bleu_loss = bleu_loss_fn(outputs, captions, caption_n_gram_count)\n","      criterion_loss = criterion(outputs.reshape(-1, vocab_size), captions[:,1:].reshape(-1))\n","\n","      val_loss = criterion_loss * (0.65+weight) + bleu_loss * (0.35-weight)\n","\n","      total_val_loss += val_loss\n","      count += 1\n","\n","    total_val_loss /= count\n","    val_losses.append(total_val_loss.item())\n","    if total_val_loss < best_val:\n","      best_val = total_val_loss\n","      save_model(model, i)\n","\n","    transformed_img, captions, _, img = next(iter(val_flickr_bleu_dataloader)) \n","    transformed_img, captions = transformed_img.to(device), captions.to(device)\n","    text_output, _ = model.generate_text(transformed_img)\n","\n","    print(weight)\n","    print(\"LR : \", lrs[-1],\"Train loss : \", total_train_loss, \"Val loss : \",total_val_loss)\n","    for i in range(5):\n","      print(\"Real: \", \"\".join([ dictionary.itos[i] for i in captions[i].cpu().detach().numpy() ]))\n","      print(\"Prodicted: \", \"\".join(text_output[i]))\n","      plt.imshow(img[i])\n","      plt.show()\n","    print()\n","\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwsf5cyJNNOo"},"outputs":[],"source":["#Way to optimize, consider the full matrix with <PAD> but assign the prob or something to be zero for ignoring\n","#Start and end might be considered"]},{"cell_type":"markdown","source":["#BLEU eval"],"metadata":{"id":"ky6vQvFxGUQf"}},{"cell_type":"code","source":["def show_image_for_testing(all_captions, text_output, img):\n","  print()\n","  print(\"Predicted: \", \"\".join(text_output))\n","  for caption in all_captions:\n","    caption = [dictionary.itos[token.item()] for token in caption]\n","    reference.append(caption)\n","    print(\"Real: \", \"\".join(caption))\n","  print(sentence_bleu(reference, text_output))\n","  plt.imshow(img)\n","  plt.show()"],"metadata":{"id":"yyJ0vDAPGZFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPaMABInHUbb"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n","avg_bleu_score = 0\n","count = 0\n","max_count = 10\n","model.eval()\n","\n","ref = list()\n","predict = list()\n","\n","\n","with torch.no_grad():\n","  for transformed_img, captions, caption_n_gram_count, img, all_captions in tqdm(test_flickr_bleu_dataloader):\n","    transformed_img, captions = transformed_img.to(device), captions.to(device)\n","    text_output, _ = model.generate_text(transformed_img)\n","    for i in range(len(all_captions)):\n","      reference = []\n","      for caption in all_captions[i]:\n","        reference.append([dictionary.itos[token.item()] for token in caption if token not in [0, 1, 2, 3]])\n","      candidate = [ token for token in text_output[i] if dictionary.stoi[token] not in [0, 1, 2, 3]]\n","\n","      ref.append(reference)\n","      predict.append(candidate)\n","\n","      bleu = sentence_bleu(reference, candidate)\n","      if bleu <0.2:\n","        show_image_for_testing(all_captions[i], text_output[i], img[i])\n"]},{"cell_type":"code","source":["print(\"Avg TEST BLEU Score:\", corpus_bleu(ref, predict))"],"metadata":{"id":"EubGQ5DGF91m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"K7kouRE_WQ5m"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"สำเนาของ anon-bleuloss-2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
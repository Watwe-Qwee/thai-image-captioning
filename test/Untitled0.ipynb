{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyOr8J931OH7P36QjjWT55Py"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMSBF_com2xK","executionInfo":{"status":"ok","timestamp":1653046816553,"user_tz":-420,"elapsed":573,"user":{"displayName":"Anon Ongsakul","userId":"12461429287531906015"}},"outputId":"b07b7e9a-7fb4-4ce9-d85b-e2e14197126a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-2., grad_fn=<MeanBackward0>)\n","x = tensor([1., 2., 3.], requires_grad=True)\n","x_clone = tensor([1., 2., 3.], grad_fn=<CloneBackward0>)\n","x_deep_copy = tensor([1., 2., 3.], requires_grad=True)\n","tensor([1., 2., 3.], requires_grad=True)\n"]}],"source":["import copy\n","import torch\n","\n","x = torch.tensor([1,2,3.]).requires_grad_()\n","x_clone = x.clone()\n","x_deep_copy = copy.deepcopy(x)\n","y = x.contiguous()\n","\n","#z = torch.mean(torch.mul(y, -1))\n","#z.backward()\n","print(z)\n","print(f'x = {x}')\n","print(f'x_clone = {x_clone}')\n","print(f'x_deep_copy = {x_deep_copy}')\n","print(y)"]},{"cell_type":"code","source":["print(z.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNKqWxcjqhCa","executionInfo":{"status":"ok","timestamp":1653046817279,"user_tz":-420,"elapsed":21,"user":{"displayName":"Anon Ongsakul","userId":"12461429287531906015"}},"outputId":"ab839105-b397-4288-dc63-2086d494a157"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["print(x.grad)\n","print(x_clone.grad)\n","print(x_deep_copy.grad)\n","print(y.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6GtmDzqjm4E4","executionInfo":{"status":"ok","timestamp":1653046817279,"user_tz":-420,"elapsed":16,"user":{"displayName":"Anon Ongsakul","userId":"12461429287531906015"}},"outputId":"36a01a45-c057-44ed-a222-a1564e653c3e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n","None\n","None\n","None\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n","  return self._grad\n"]}]},{"cell_type":"code","source":["x2 = y[1:3] * y[2:4]\n","x2 = torch.mean(x2)\n","x2.backward()"],"metadata":{"id":"aGA4K3jDo1MX","executionInfo":{"status":"ok","timestamp":1653046817282,"user_tz":-420,"elapsed":15,"user":{"displayName":"Anon Ongsakul","userId":"12461429287531906015"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print(x.grad)\n","print(x_clone.grad)\n","print(x_deep_copy.grad)\n","print(y.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSWJstnmOkt6","executionInfo":{"status":"ok","timestamp":1653046817283,"user_tz":-420,"elapsed":15,"user":{"displayName":"Anon Ongsakul","userId":"12461429287531906015"}},"outputId":"6090cf89-656a-4292-d79b-2110a2d63832"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0000, 1.5000, 4.0000])\n","None\n","None\n","tensor([0.0000, 1.5000, 4.0000])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n","  return self._grad\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"oSR5utWvPB7B","executionInfo":{"status":"ok","timestamp":1653046817288,"user_tz":-420,"elapsed":15,"user":{"displayName":"Anon Ongsakul","userId":"12461429287531906015"}}},"execution_count":20,"outputs":[]}]}